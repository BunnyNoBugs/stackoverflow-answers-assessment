{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bi-Encoder ranking model",
   "id": "9bc0d925463af42e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading",
   "id": "f83bc7af753be01f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:41.827699Z",
     "start_time": "2025-05-16T12:15:41.110218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "from src.utils.config_management import CONFIG"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:42.084637Z",
     "start_time": "2025-05-16T12:15:41.866235Z"
    }
   },
   "cell_type": "code",
   "source": "hf_dataset = load_from_disk(CONFIG['paths']['data']['dalip_hf_dataset'])",
   "id": "8b860895efd9c9e6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:42.209653Z",
     "start_time": "2025-05-16T12:15:42.204116Z"
    }
   },
   "cell_type": "code",
   "source": "hf_dataset",
   "id": "1d2094a8ab250814",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer_id', 'question_id', 'answer_creation_date', 'answer_score', 'answer_normalized_score', 'answer_log_normalized_score', 'answer_body', 'answer_last_edit_date', 'answer_last_activity_date', 'answer_comment_count', 'answer_community_owned_date', 'question_creation_date', 'question_score', 'question_view_count', 'question_body', 'question_last_edit_date', 'question_last_activity_date', 'question_title', 'question_tags', 'question_answer_count', 'question_comment_count', 'question_favorite_count', 'question_closed_date', 'question_community_owned_date', 'answer_accepted'],\n",
       "        num_rows: 42700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answer_id', 'question_id', 'answer_creation_date', 'answer_score', 'answer_normalized_score', 'answer_log_normalized_score', 'answer_body', 'answer_last_edit_date', 'answer_last_activity_date', 'answer_comment_count', 'answer_community_owned_date', 'question_creation_date', 'question_score', 'question_view_count', 'question_body', 'question_last_edit_date', 'question_last_activity_date', 'question_title', 'question_tags', 'question_answer_count', 'question_comment_count', 'question_favorite_count', 'question_closed_date', 'question_community_owned_date', 'answer_accepted'],\n",
       "        num_rows: 10563\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data preprocessing",
   "id": "d8d1b6fc0ab91a4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:42.321972Z",
     "start_time": "2025-05-16T12:15:42.234102Z"
    }
   },
   "cell_type": "code",
   "source": "from src.utils.text_preprocessing import Preprocessor",
   "id": "bde6798d564e3ca4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:42.351565Z",
     "start_time": "2025-05-16T12:15:42.346875Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessor = Preprocessor(preserve_html_tags=['code'])",
   "id": "19aa545a200e9da4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:42.528826Z",
     "start_time": "2025-05-16T12:15:42.373960Z"
    }
   },
   "cell_type": "code",
   "source": "hf_dataset = hf_dataset.map(preprocessor, batched=True)",
   "id": "c31946c038001583",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:42.548255Z",
     "start_time": "2025-05-16T12:15:42.544839Z"
    }
   },
   "cell_type": "code",
   "source": "hf_dataset",
   "id": "bb45c826ddfb2120",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer_id', 'question_id', 'answer_creation_date', 'answer_score', 'answer_normalized_score', 'answer_log_normalized_score', 'answer_body', 'answer_last_edit_date', 'answer_last_activity_date', 'answer_comment_count', 'answer_community_owned_date', 'question_creation_date', 'question_score', 'question_view_count', 'question_body', 'question_last_edit_date', 'question_last_activity_date', 'question_title', 'question_tags', 'question_answer_count', 'question_comment_count', 'question_favorite_count', 'question_closed_date', 'question_community_owned_date', 'answer_accepted', 'question_text', 'answer_text'],\n",
       "        num_rows: 42700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answer_id', 'question_id', 'answer_creation_date', 'answer_score', 'answer_normalized_score', 'answer_log_normalized_score', 'answer_body', 'answer_last_edit_date', 'answer_last_activity_date', 'answer_comment_count', 'answer_community_owned_date', 'question_creation_date', 'question_score', 'question_view_count', 'question_body', 'question_last_edit_date', 'question_last_activity_date', 'question_title', 'question_tags', 'question_answer_count', 'question_comment_count', 'question_favorite_count', 'question_closed_date', 'question_community_owned_date', 'answer_accepted', 'question_text', 'answer_text'],\n",
       "        num_rows: 10563\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-tuning",
   "id": "88b83882127e4b27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create pairs dataset",
   "id": "21e9c52db3f1c696"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:45.774622Z",
     "start_time": "2025-05-16T12:15:42.580071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Literal, Union\n",
    "from datasets import Dataset\n",
    "from itertools import combinations\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math"
   ],
   "id": "62a440ec4d3488ae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:45.946463Z",
     "start_time": "2025-05-16T12:15:45.789652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "TARGET_COL = 'answer_normalized_score'\n",
    "PAIRS_SAMPLING_STRATEGY = 'mean'\n",
    "N_SAMPLES = 10\n",
    "MODEL_PATH = 'mmukh/SOBertBase'\n",
    "MODEL_NAME = MODEL_PATH.split('/')[-1]\n",
    "MAX_LENGTH = 1024\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = math.ceil(4 / BATCH_SIZE)\n",
    "EMBEDDINGS_POOLING = 'mean'\n",
    "LOSS = 'margin_ranking_loss'\n",
    "MODEL_OUTPUT_PATH = os.path.join(CONFIG['paths']['models']['dalip_bi-encoder_ranking'],\n",
    "                                 f'bi-encoder_ranking_{MODEL_NAME}_{EMBEDDINGS_POOLING}')"
   ],
   "id": "57ff34458f741481",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:45.973371Z",
     "start_time": "2025-05-16T12:15:45.964294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_pairs_dataset_df(dataset_df,\n",
    "                         pairs_sampling_strategy: Union[Literal['mean'], Literal['topk']] = 'mean',\n",
    "                         n: Union[int, Literal['all']] = 'all',\n",
    "                         ) -> pd.DataFrame:\n",
    "    def create_pairs(group, pairs_sampling_strategy, n):\n",
    "        group = group.sort_values(TARGET_COL, ascending=False)\n",
    "\n",
    "        all_pairs_idxs = list(combinations(group.index, 2))\n",
    "        pairs_idxs = []\n",
    "\n",
    "        if n == 'all':\n",
    "            for pair_idx in all_pairs_idxs:\n",
    "                if group.loc[pair_idx[0]][TARGET_COL] != group.loc[pair_idx[1]][TARGET_COL]:\n",
    "                    pairs_idxs.append(pair_idx)\n",
    "\n",
    "        else:\n",
    "            if pairs_sampling_strategy == 'mean':\n",
    "                random.shuffle(all_pairs_idxs)\n",
    "                for pair_idx in all_pairs_idxs:\n",
    "                    if group.loc[pair_idx[0]][TARGET_COL] != group.loc[pair_idx[1]][TARGET_COL]:\n",
    "                        pairs_idxs.append(pair_idx)\n",
    "                    if len(pairs_idxs) == n:\n",
    "                        break\n",
    "\n",
    "            elif pairs_sampling_strategy == 'topk':\n",
    "                for curr_k in range(min(n, len(group))):\n",
    "                    anchor_idx = group.index[curr_k]\n",
    "                    for idx in group.index[curr_k:]:\n",
    "                        if group.loc[anchor_idx][TARGET_COL] != group.loc[idx][TARGET_COL]:\n",
    "                            pairs_idxs.append((anchor_idx, idx))\n",
    "\n",
    "        pairs = []\n",
    "        for pair_idx in pairs_idxs:\n",
    "            pair = {\n",
    "                'question_id': group.loc[pair_idx[0]]['question_id'],\n",
    "                'answer_1_id': group.loc[pair_idx[0]]['answer_id'],\n",
    "                'answer_2_id': group.loc[pair_idx[1]]['answer_id'],\n",
    "                'question_text': group.loc[pair_idx[0]]['question_text'],\n",
    "                'answer_1_text': group.loc[pair_idx[0]]['answer_text'],\n",
    "                'answer_2_text': group.loc[pair_idx[1]]['answer_text'],\n",
    "                f'answer_1_{TARGET_COL}': group.loc[pair_idx[0]][TARGET_COL],\n",
    "                f'answer_2_{TARGET_COL}': group.loc[pair_idx[1]][TARGET_COL],\n",
    "            }\n",
    "            if group.loc[pair_idx[0]][TARGET_COL] > group.loc[pair_idx[1]][TARGET_COL]:\n",
    "                pair['label'] = 1\n",
    "            else:\n",
    "                pair['label'] = -1\n",
    "\n",
    "            pairs.append(pair)\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    groups = dataset_df.groupby('question_id')\n",
    "\n",
    "    pairs_dataset_df = []\n",
    "    for name, group in tqdm(groups):\n",
    "        group_pairs = create_pairs(group, pairs_sampling_strategy, n)\n",
    "        pairs_dataset_df.extend(group_pairs)\n",
    "    pairs_dataset_df = pd.DataFrame(pairs_dataset_df)\n",
    "\n",
    "    return pairs_dataset_df"
   ],
   "id": "772cbbb4e399c1ac",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:15:59.109515Z",
     "start_time": "2025-05-16T12:15:45.989550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_df = pd.DataFrame(hf_dataset['train'])\n",
    "test_dataset_df = pd.DataFrame(hf_dataset['test'])"
   ],
   "id": "defcbe1f289d4ed0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:01.508667Z",
     "start_time": "2025-05-16T12:15:59.146186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_pairs_dataset_df = create_pairs_dataset_df(train_dataset_df, pairs_sampling_strategy=PAIRS_SAMPLING_STRATEGY, n=N_SAMPLES)\n",
    "test_pairs_dataset_df = create_pairs_dataset_df(test_dataset_df, pairs_sampling_strategy='mean', n='all')\n",
    "\n",
    "hf_dataset['test'] = hf_dataset['test'].rename_column('answer_text', 'answer_1_text')"
   ],
   "id": "16382c3c29ca7bb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7776/7776 [00:41<00:00, 186.85it/s]\n",
      "100%|██████████| 1945/1945 [00:19<00:00, 98.65it/s] \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:02.156579Z",
     "start_time": "2025-05-16T12:17:01.639238Z"
    }
   },
   "cell_type": "code",
   "source": "train_pairs_dataset = Dataset.from_pandas(train_pairs_dataset_df)",
   "id": "22bfc4ddc0be7812",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define model and data collator",
   "id": "9821be29bb5c083a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:11.077648Z",
     "start_time": "2025-05-16T12:17:02.193064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedTokenizerBase, MegatronBertModel, PreTrainedTokenizerFast"
   ],
   "id": "efcec28b9176977a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:17:07.133952: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 12:17:07.779017: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-16 12:17:07.779086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-16 12:17:07.884323: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 12:17:08.104983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 12:17:09.705019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:12.438769Z",
     "start_time": "2025-05-16T12:17:11.110208Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_model = MegatronBertModel.from_pretrained(MODEL_PATH)",
   "id": "b21b0cc65abe79b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MegatronBertModel were not initialized from the model checkpoint at mmukh/SOBertBase and are newly initialized: ['embeddings.token_type_embeddings.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:12.905349Z",
     "start_time": "2025-05-16T12:17:12.539167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "if not tokenizer.pad_token:\n",
    "    pad_token_id = encoder_model.embeddings.word_embeddings.padding_idx\n",
    "    print(f'Setting pad token id to {pad_token_id}...')\n",
    "    tokenizer.pad_token_id = pad_token_id\n",
    "    print(f'Pad token set to {tokenizer.pad_token}')"
   ],
   "id": "ff64a7acfaa4b9e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token id to 0...\n",
      "Pad token set to <unk>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:13.017529Z",
     "start_time": "2025-05-16T12:17:13.008389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiEncoderRanker(nn.Module):\n",
    "    def __init__(self, encoder_model, embeddings_pooling):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model\n",
    "        self.embeddings_pooling = embeddings_pooling\n",
    "        self.hidden_size = self.encoder.embeddings.word_embeddings.embedding_dim\n",
    "\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def get_sentence_embeddings(self, tokenized_inputs):\n",
    "        outputs = self.encoder(**tokenized_inputs)\n",
    "\n",
    "        attention_mask = tokenized_inputs['attention_mask']\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        if self.embeddings_pooling == 'mean':\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "            sum_embeddings = (last_hidden_state * attention_mask_expanded).sum(dim=1)\n",
    "            sum_mask = attention_mask_expanded.sum(dim=1)\n",
    "            pooled = sum_embeddings / sum_mask\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def gradient_checkpointing_enable(self, **gradient_checkpointing_kwargs):\n",
    "        if hasattr(self.encoder, 'gradient_checkpointing_enable'):\n",
    "            self.encoder.gradient_checkpointing_enable()\n",
    "        else:\n",
    "            raise NotImplementedError('Encoder model does not support gradient checkpointing.')\n",
    "\n",
    "    def gradient_checkpointing_disable(self, **gradient_checkpointing_kwargs):\n",
    "        if hasattr(self.encoder, 'gradient_checkpointing_disable'):\n",
    "            self.encoder.gradient_checkpointing_disable()\n",
    "        else:\n",
    "            raise NotImplementedError('Encoder model does not support gradient checkpointing.')\n",
    "\n",
    "    def forward(self, questions_tokenized, answers_1_tokenized, answers_2_tokenized=None, labels=None):\n",
    "        question_embeddings = self.get_sentence_embeddings(questions_tokenized)\n",
    "        answer_1_embeddings = self.get_sentence_embeddings(answers_1_tokenized)\n",
    "        combined_1 = torch.cat([question_embeddings, answer_1_embeddings], dim=1)\n",
    "        answer_1_scores = self.scorer(combined_1).squeeze(-1)\n",
    "\n",
    "        outputs = {'answer_1_scores': answer_1_scores}\n",
    "\n",
    "        if answers_2_tokenized is not None:\n",
    "            answer_2_embeddings = self.get_sentence_embeddings(answers_2_tokenized)\n",
    "            combined_2 = torch.cat([question_embeddings, answer_2_embeddings], dim=1)\n",
    "            answer_2_scores = self.scorer(combined_2).squeeze(-1)\n",
    "\n",
    "            outputs['answer_2_scores'] = answer_2_scores\n",
    "\n",
    "        return outputs"
   ],
   "id": "c9aa43cf5bbf29c0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:13.405836Z",
     "start_time": "2025-05-16T12:17:13.054308Z"
    }
   },
   "cell_type": "code",
   "source": "model = BiEncoderRanker(encoder_model, embeddings_pooling=EMBEDDINGS_POOLING).to(device)",
   "id": "d7f079ef6bccaa06",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:13.451123Z",
     "start_time": "2025-05-16T12:17:13.444993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class BiEncoderPairwiseDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: bool = True\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        question_texts = []\n",
    "        answer_1_texts = []\n",
    "        answer_2_texts = []\n",
    "        labels = []\n",
    "\n",
    "        for sample in batch:\n",
    "            question_texts.append(sample['question_text'])\n",
    "            answer_1_texts.append(sample['answer_1_text'])\n",
    "            if 'answer_2_text' in sample:\n",
    "                answer_2_texts.append(sample['answer_2_text'])\n",
    "            if 'label' in sample:  # if training\n",
    "                labels.append(sample['label'])\n",
    "            else:  # if evaluation\n",
    "                labels.append(sample[TARGET_COL])\n",
    "\n",
    "        questions_tokenized = self.tokenizer(question_texts, padding=self.padding, truncation=True, max_length=MAX_LENGTH,\n",
    "                                             return_tensors='pt')\n",
    "        answers_1_tokenized = self.tokenizer(answer_1_texts, padding=self.padding, truncation=True, max_length=MAX_LENGTH,\n",
    "                                           return_tensors='pt')\n",
    "\n",
    "        labels = torch.tensor(labels).float()\n",
    "\n",
    "        collated_batch = {\n",
    "            'questions_tokenized': questions_tokenized,\n",
    "            'answers_1_tokenized': answers_1_tokenized,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        if answer_2_texts:\n",
    "            answers_2_tokenized = self.tokenizer(answer_2_texts, padding=self.padding, truncation=True, max_length=MAX_LENGTH,\n",
    "                                           return_tensors='pt')\n",
    "\n",
    "            collated_batch['answers_2_tokenized'] = answers_2_tokenized\n",
    "\n",
    "        return collated_batch"
   ],
   "id": "52e7f96bec647b9a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:13.488621Z",
     "start_time": "2025-05-16T12:17:13.486087Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = BiEncoderPairwiseDataCollator(tokenizer=tokenizer)",
   "id": "9730574ce560bb05",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model",
   "id": "4caf2725e7e0e2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:14.813723Z",
     "start_time": "2025-05-16T12:17:13.523805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from src.evaluation import RankingEvaluator\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import math"
   ],
   "id": "3566b32abb0d4ea1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:14.853719Z",
     "start_time": "2025-05-16T12:17:14.850200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if LOSS == 'margin_ranking_loss':\n",
    "    loss_fn = nn.MarginRankingLoss(margin=1.0)\n",
    "\n",
    "def trainer_loss_fn(outputs, labels, num_items_in_batch=None):\n",
    "    answer_1_scores = outputs['answer_1_scores']\n",
    "\n",
    "    if 'answer_2_scores' in outputs: # if training\n",
    "        answer_2_scores = outputs['answer_2_scores']\n",
    "        loss = loss_fn(answer_1_scores, answer_2_scores, labels)\n",
    "\n",
    "    else: # if evaluation\n",
    "        loss = torch.tensor(0.0)\n",
    "\n",
    "    return loss"
   ],
   "id": "114ac89ab3b0dc94",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:14.898921Z",
     "start_time": "2025-05-16T12:17:14.888041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_question_ids = hf_dataset['test']['question_id']\n",
    "\n",
    "evaluator = RankingEvaluator(ndcg_k=list(range(1, 11)),\n",
    "                             ndcg_gain_func='exponential', ndcg_discount_func='logarithmic')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    predictions_df = pd.DataFrame()\n",
    "    predictions_df['answer_id'] = hf_dataset['test']['answer_id']\n",
    "    predictions_df[TARGET_COL] = hf_dataset['test'][TARGET_COL]\n",
    "    predictions_df = predictions_df[:len(predictions)]\n",
    "    predictions_df['predicted_score'] = predictions\n",
    "\n",
    "    pairs_predictions_df = test_pairs_dataset_df.merge(predictions_df, left_on='answer_1_id', right_on='answer_id')\n",
    "    pairs_predictions_df = pairs_predictions_df.rename(columns={'predicted_score': 'answer_1_predicted_score'})\n",
    "    pairs_predictions_df = pairs_predictions_df.merge(predictions_df, left_on='answer_2_id', right_on='answer_id')\n",
    "    pairs_predictions_df = pairs_predictions_df.rename(columns={'predicted_score': 'answer_2_predicted_score'})\n",
    "\n",
    "    loss = loss_fn(torch.tensor(pairs_predictions_df['answer_1_predicted_score']),\n",
    "                   torch.tensor(pairs_predictions_df['answer_2_predicted_score']),\n",
    "                   torch.tensor(pairs_predictions_df['label']))\n",
    "\n",
    "    metrics = {LOSS: loss}\n",
    "    metrics.update(evaluator(labels, predictions, test_question_ids))\n",
    "    metrics.pop('mae')\n",
    "\n",
    "    wandb.log({'predictions_table': wandb.Table(dataframe=predictions_df)})\n",
    "\n",
    "    return metrics"
   ],
   "id": "dd7c4401d56759b1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:14.959484Z",
     "start_time": "2025-05-16T12:17:14.936224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    logging_steps=1,\n",
    "    eval_steps=int(len(train_pairs_dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS / 10),\n",
    "    eval_strategy = \"steps\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to='wandb',\n",
    "    remove_unused_columns=False,\n",
    "    # gradient_checkpointing=True,\n",
    "    # optim=\"adamw_8bit\"\n",
    ")"
   ],
   "id": "a15993a3fda30eb3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:17:16.953868Z",
     "start_time": "2025-05-16T12:17:15.020579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run = wandb.init(\n",
    "    project='dalip-stackoverflow-answer-ranking',\n",
    "    tags=['bi-encoder', 'ranking']\n",
    ")\n",
    "\n",
    "wandb.config.update({\n",
    "    'preprocessing': preprocessor.__dict__,\n",
    "    'dataset': {\n",
    "        'pairs_sampling_strategy': PAIRS_SAMPLING_STRATEGY,\n",
    "        'n': N_SAMPLES\n",
    "    },\n",
    "    'model_name': MODEL_NAME,\n",
    "    'vectorizer': {\n",
    "        'vectorization_type': 'embeddings',\n",
    "        'embeddings_pooling': EMBEDDINGS_POOLING,\n",
    "        'max_length': MAX_LENGTH\n",
    "    },\n",
    "    'loss_fn': LOSS\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_pairs_dataset,\n",
    "    eval_dataset=hf_dataset['test'],\n",
    "    compute_loss_func=trainer_loss_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "2809710310e5673f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbunnynobugs\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/stackoverflow-answer-assessment/wandb/run-20250516_121716-fy2r99gc</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bunnynobugs/dalip-stackoverflow-answer-ranking/runs/fy2r99gc' target=\"_blank\">graceful-fog-137</a></strong> to <a href='https://wandb.ai/bunnynobugs/dalip-stackoverflow-answer-ranking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/bunnynobugs/dalip-stackoverflow-answer-ranking' target=\"_blank\">https://wandb.ai/bunnynobugs/dalip-stackoverflow-answer-ranking</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/bunnynobugs/dalip-stackoverflow-answer-ranking/runs/fy2r99gc' target=\"_blank\">https://wandb.ai/bunnynobugs/dalip-stackoverflow-answer-ranking/runs/fy2r99gc</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-16T12:44:58.959952500Z",
     "start_time": "2025-05-16T12:17:17.040914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "run.finish()"
   ],
   "id": "52189f515767bc0b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2414' max='65835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2414/65835 28:59 < 12:42:23, 1.39 it/s, Epoch 0.18/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Margin Ranking Loss</th>\n",
       "      <th>Ndcg@1 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@2 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@3 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@4 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@5 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@6 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@7 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@8 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@9 G.exponential D.logarithmic</th>\n",
       "      <th>Ndcg@10 G.exponential D.logarithmic</th>\n",
       "      <th>Hit Rate@1</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>4.189300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844493</td>\n",
       "      <td>0.520546</td>\n",
       "      <td>0.622459</td>\n",
       "      <td>0.700874</td>\n",
       "      <td>0.755762</td>\n",
       "      <td>0.777599</td>\n",
       "      <td>0.786951</td>\n",
       "      <td>0.791173</td>\n",
       "      <td>0.793832</td>\n",
       "      <td>0.794831</td>\n",
       "      <td>0.795371</td>\n",
       "      <td>0.401542</td>\n",
       "      <td>341.432800</td>\n",
       "      <td>30.937000</td>\n",
       "      <td>15.470000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
